{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# RAG Failure #13: The Structural Influence Blindness\n",
    "\n",
    "## The Problem\n",
    "RAG retrieves text based on **Semantic Relevance**. It excels at finding *facts* (\"Who is the manager?\") but fails at finding **structural patterns** (\"Who is the bottleneck?\"). \n",
    "\n",
    "Even if you feed an LLM **every single email log** (100% Retrieval), it struggles to mentally map the aggregate flow of information. It biases heavily towards explicit titles (VP, Director) found in Org Charts, ignoring the \"Hidden Influencer\" who actually bridges the teams.\n",
    "\n",
    "## The Scenario: Project Omega Communication Breakdown\n",
    "**Query:** \"Who is the single point of failure (bottleneck) for information flow in Project Omega?\"\n",
    "\n",
    "**The Data (Org Chart vs. Reality):**\n",
    "1.  **Doc 1 (Org Chart):** \"**Alice** is the Senior Project Manager. She leads the strategy.\"\n",
    "2.  **Doc 2 (Org Chart):** \"**Bob** is the VP of Engineering. He approves budget.\"\n",
    "3.  **Docs 3-8 (Email Logs):** \n",
    "    -   The Dev Team (Dave, Eve) emails **Carol** for everything.\n",
    "    -   The QA Team (Frank) emails **Carol** for bugs.\n",
    "    -   The Management (Alice, Bob) emails **Carol** for status updates.\n",
    "    -   *Crucially:* Alice never emails Dave/Eve directly. Bob never emails Frank directly.\n",
    "\n",
    "**Naive RAG Failure (Even with Full Context):** \n",
    "The LLM reads Doc 1 (\"Alice is PM\") and Doc 2 (\"Bob is VP\"). It reads the emails but treats them as \"noise\". \n",
    "Answer: *\"Alice is the bottleneck because she is the Project Manager responsible for the project.\"* (A Semantic hallucination based on job description).\n",
    "\n",
    "**KG Solution:** We build a **Communication Graph**. We run **Betweenness Centrality**. We prove mathematically that Carol is the bridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Environment Setup ---\n",
    "!pip install -q langchain langchain-community langchain-huggingface faiss-cpu networkx transformers sentence-transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyLlama-1.1B-Chat-v1.0...\n",
      "Model loaded. Pipeline ready.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "import networkx as nx\n",
    "\n",
    "# --- Step 2: Load Model ---\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    temperature=0.1, \n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded. Pipeline ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_sim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8 Documents.\n",
      "Doc 1: [HR Org Chart] Alice is the designated Project Manager for Project Omega. She holds the highest authority and responsibility for delivery.\n",
      "Doc 2: [HR Org Chart] Bob is the VP of Engineering. He oversees the budget for Project Omega and has veto power.\n",
      "Doc 3: [Email Log] From: Dave (Dev) | To: Carol | Subject: Need API Specs. Hey Carol, can you unblock me on the API?\n",
      "Doc 4: [Email Log] From: Eve (Dev) | To: Carol | Subject: DB Schema. Carol, is the schema ready?\n",
      "Doc 5: [Email Log] From: Alice (PM) | To: Carol | Subject: Status Update. Carol, please summarize the dev team's progress for me.\n",
      "Doc 6: [Email Log] From: Bob (VP) | To: Carol | Subject: Budget Review. Carol, explain the server costs.\n",
      "Doc 7: [Email Log] From: Frank (QA) | To: Carol | Subject: Bug Report. Carol, does this look right?\n",
      "Doc 8: [Email Log] From: Carol | To: Alice | Subject: RE: Status. Here is the compiled report from everyone.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# --- Step 3: Simulate Logs & Org Charts ---\n",
    "raw_texts = [\n",
    "    \"[HR Org Chart] Alice is the designated Project Manager for Project Omega. She holds the highest authority and responsibility for delivery.\",\n",
    "    \"[HR Org Chart] Bob is the VP of Engineering. He oversees the budget for Project Omega and has veto power.\",\n",
    "    \"[Email Log] From: Dave (Dev) | To: Carol | Subject: Need API Specs. Hey Carol, can you unblock me on the API?\",\n",
    "    \"[Email Log] From: Eve (Dev) | To: Carol | Subject: DB Schema. Carol, is the schema ready?\",\n",
    "    \"[Email Log] From: Alice (PM) | To: Carol | Subject: Status Update. Carol, please summarize the dev team's progress for me.\",\n",
    "    \"[Email Log] From: Bob (VP) | To: Carol | Subject: Budget Review. Carol, explain the server costs.\",\n",
    "    \"[Email Log] From: Frank (QA) | To: Carol | Subject: Bug Report. Carol, does this look right?\",\n",
    "    \"[Email Log] From: Carol | To: Alice | Subject: RE: Status. Here is the compiled report from everyone.\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=t) for t in raw_texts]\n",
    "print(f\"Created {len(docs)} Documents.\")\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"Doc {i+1}: {d.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive_rag_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NAIVE RAG (Full Context Failure) ---\n",
      "Query: Who is the single point of failure (bottleneck) for information flow in Project Omega?\n",
      "\n",
      "Retrieved Context (k=8 - ALL DOCS):\n",
      "1. [HR Org Chart] Alice is the designated Project Manager for Project Omega. She holds the highest authority and responsibility for delivery.\n",
      "2. [HR Org Chart] Bob is the VP of Engineering. He oversees the budget for Project Omega and has veto power.\n",
      "3. [Email Log] From: Dave (Dev) | To: Carol | Subject: Need API Specs. Hey Carol, can you unblock me on the API?\n",
      "4. [Email Log] From: Eve (Dev) | To: Carol | Subject: DB Schema. Carol, is the schema ready?\n",
      "5. [Email Log] From: Alice (PM) | To: Carol | Subject: Status Update. Carol, please summarize the dev team's progress for me.\n",
      "6. [Email Log] From: Bob (VP) | To: Carol | Subject: Budget Review. Carol, explain the server costs.\n",
      "7. [Email Log] From: Frank (QA) | To: Carol | Subject: Bug Report. Carol, does this look right?\n",
      "8. [Email Log] From: Carol | To: Alice | Subject: RE: Status. Here is the compiled report from everyone.\n",
      "\n",
      "LLM Answer:\n",
      "Based on the context, Alice is the single point of failure because she is the designated Project Manager and holds the highest authority and responsibility for delivery. Bob, the VP of Engineering, also holds significant authority.\n",
      "\n",
      "ANALYSIS:\n",
      "FAILURE. The LLM read all the emails but ignored the pattern. \n",
      "It biased towards the explicit definition of \"Authority\" in Doc 1. \n",
      "It failed to notice that Alice CANNOT do her job without Carol, because Alice never speaks to the Devs directly.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Naive RAG (Full Retrieval) ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"\\n--- NAIVE RAG (Full Context Failure) ---\")\n",
    "query = \"Who is the single point of failure (bottleneck) for information flow in Project Omega?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 1. Indexing\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 2. Retrieval\n",
    "# We retrieve ALL documents. The problem is structural cognition, not recall.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"\\nRetrieved Context (k=8 - ALL DOCS):\")\n",
    "context_str = \"\"\n",
    "for i, d in enumerate(retrieved_docs):\n",
    "    print(f\"{i+1}. {d.page_content}\")\n",
    "    context_str += d.page_content + \"\\n\"\n",
    "\n",
    "# 3. Generation\n",
    "prompt = f\"<|system|>\\nAnswer based on context.\\n<|user|>\\nContext:\\n{context_str}\\nQuestion:\\n{query}\\n<|assistant|>\"\n",
    "response = llm.invoke(prompt)\n",
    "cleaned_response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "print(\"\\nLLM Answer:\")\n",
    "print(cleaned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg_construction_weighted",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- WEIGHTED INTERACTION GRAPH ---\n",
      "\n",
      "Processing Log: [Email Log] From: Dave (Dev) | To: Carol | Subject: Need API Specs. Hey Carol, can you unblock me on the API?\n",
      "   [Extracted]: Dave (Dev) -> Carol\n",
      "   [Graph]: Added edge. Current Weight: 1\n",
      "\n",
      "Processing Log: [Email Log] From: Eve (Dev) | To: Carol | Subject: DB Schema. Carol, is the schema ready?\n",
      "   [Extracted]: Eve (Dev) -> Carol\n",
      "   [Graph]: Added edge. Current Weight: 1\n",
      "\n",
      "Processing Log: [Email Log] From: Alice (PM) | To: Carol | Subject: Status Update. Carol, please summarize the dev team's progress for me.\n",
      "   [Extracted]: Alice (PM) -> Carol\n",
      "   [Graph]: Added edge. Current Weight: 1\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Network Construction (Weighted Graph) ---\n",
    "# We build a graph where every email increases the 'Connection Strength'.\n",
    "\n",
    "kg = nx.Graph() # Undirected for interaction analysis\n",
    "\n",
    "def extract_interaction(text):\n",
    "    \"\"\"\n",
    "    Extracts Sender and Receiver.\n",
    "    \"\"\"\n",
    "    if \"[Email Log]\" not in text: return []\n",
    "    \n",
    "    prompt = f\"\"\"<|system|>\n",
    "    Extract the Sender and Receiver names.\n",
    "    Format: Sender | EMAILED | Receiver\n",
    "    <|user|>\n",
    "    Text: {text}\n",
    "    <|assistant|>\"\"\"\n",
    "    \n",
    "    raw = llm.invoke(prompt)\n",
    "    out = raw.split(\"<|assistant|>\")[-1].strip()\n",
    "    if \"|\" in out:\n",
    "        return [p.strip() for p in out.split(\"|\")]\n",
    "    return []\n",
    "\n",
    "print(\"\\n--- WEIGHTED INTERACTION GRAPH ---\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"\\nProcessing Log: {doc.page_content}\")\n",
    "    parts = extract_interaction(doc.page_content)\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        sender, rel, receiver = parts[0], parts[1], parts[2]\n",
    "        print(f\"   [Extracted]: {sender} -> {receiver}\")\n",
    "        \n",
    "        # Add/Update Weighted Edge\n",
    "        if kg.has_edge(sender, receiver):\n",
    "            kg[sender][receiver]['weight'] += 1\n",
    "        else:\n",
    "            kg.add_edge(sender, receiver, weight=1)\n",
    "            \n",
    "        print(f\"   [Graph]: Added edge. Current Weight: {kg[sender][receiver]['weight']}\")\n",
    "    else:\n",
    "        print(\"   [Skipped]: Not a communication log.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg_solution_centrality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BETWEENNESS CENTRALITY ANALYSIS ---\n",
      "Query: \"Who is the single point of failure...\"\n",
      "\n",
      "Running Mathematical Analysis on Graph Topology...\n",
      "\n",
      "Centrality Scores (0.0 to 1.0):\n",
      "   1. Carol: 0.90\n",
      "   2. Alice (PM): 0.00\n",
      "   3. Bob (VP): 0.00\n",
      "   4. Dave (Dev): 0.00\n",
      "   5. Eve (Dev): 0.00\n",
      "   6. Frank (QA): 0.00\n",
      "\n",
      "Topology Insight:\n",
      "- Alice (PM) connects ONLY to Carol.\n",
      "- The Dev Team connects ONLY to Carol.\n",
      "- Therefore, if Carol is removed, the graph splits into 5 disconnected components.\n",
      "\n",
      "Final Answer (Generated from Graph Metrics):\n",
      "Network analysis identifies Carol as the critical bottleneck with a Betweenness Centrality of 0.90. She is the only bridge connecting Management (Alice, Bob) to the Operational Teams (Dev, QA).\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: The Solution (Centrality Analysis) ---\n",
    "# We prove the bottleneck using Math.\n",
    "\n",
    "print(\"\\n--- BETWEENNESS CENTRALITY ANALYSIS ---\")\n",
    "print(f\"Query: \\\"{query}\\\"\")\n",
    "\n",
    "def analyze_network():\n",
    "    print(\"\\nRunning Mathematical Analysis on Graph Topology...\")\n",
    "    \n",
    "    # Betweenness Centrality: Measures how often a node acts as a bridge along the shortest path.\n",
    "    centrality = nx.betweenness_centrality(kg, weight='weight')\n",
    "    \n",
    "    sorted_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nCentrality Scores (0.0 to 1.0):\")\n",
    "    for node, score in sorted_nodes:\n",
    "        print(f\"   {node}: {score:.2f}\")\n",
    "        \n",
    "    top_person = sorted_nodes[0][0]\n",
    "    top_score = sorted_nodes[0][1]\n",
    "    \n",
    "    print(\"\\nTopology Insight:\")\n",
    "    print(f\"- Alice (PM) connects ONLY to {top_person}.\")\n",
    "    print(f\"- The Dev Team connects ONLY to {top_person}.\")\n",
    "    print(f\"- Therefore, if {top_person} is removed, the graph splits into 5 disconnected components.\")\n",
    "    \n",
    "    return f\"Network analysis identifies {top_person} as the critical bottleneck with a Betweenness Centrality of {top_score:.2f}. She is the only bridge connecting Management (Alice, Bob) to the Operational Teams (Dev, QA).\"\n",
    "\n",
    "final_answer = analyze_network()\n",
    "\n",
    "print(f\"\\nFinal Answer (Generated from Graph Metrics):\\n{final_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
