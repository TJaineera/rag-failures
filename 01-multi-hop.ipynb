{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# RAG Failure #1: The Multi-Hop Disconnect\n",
    "\n",
    "## The Problem\n",
    "Standard RAG retrieves documents based on semantic similarity to the query. If the answer requires connecting **Fact A $\\to$ Fact B $\\to$ Fact C**, and Fact C is semantically unrelated to the original query, RAG fails.\n",
    "\n",
    "## The Scenario: Corporate Intelligence Investigation\n",
    "**Query:** \"What is the primary currency used in the city where the lead engineer of Project Chimera was born?\"\n",
    "\n",
    "**The Logic Chain (Hidden in disjointed docs):**\n",
    "1.  **Doc 1:** Project Chimera is led by **Dr. Elias Thorne**.\n",
    "2.  **Doc 2:** Dr. Elias Thorne was born in **Valoria City**.\n",
    "3.  **Doc 3:** Valoria City uses the **Valorian Credit (V-Cred)** as its currency.\n",
    "\n",
    "**The Adversarial Noise:**\n",
    "-   Mention of \"Project Chimera-Next\" (a different project).\n",
    "-   Mention of \"Valoria\" in a tourism context (irrelevant).\n",
    "-   Mention of other currencies (Euro, USD) in irrelevant docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Environment Setup ---\n",
    "# Installing specific versions to ensure compatibility in Colab/Jupyter\n",
    "!pip install -q langchain langchain-community langchain-huggingface faiss-cpu networkx transformers sentence-transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyLlama-1.1B-Chat-v1.0...\n",
      "Model loaded. Pipeline ready.\n",
      "Embedding model loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "import networkx as nx\n",
    "\n",
    "# --- Step 2: Load LLM & Embeddings ---\n",
    "# We use TinyLlama 1.1B. It's small, fast, and follows instructions well.\n",
    "# This avoids 'HuggingFace Login' issues while providing Llama-level architecture.\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Create a text-generation pipeline\n",
    "# temperature=0.1 ensures the Extraction phase is deterministic and not 'creative'\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    temperature=0.1,  \n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Model loaded. Pipeline ready.\")\n",
    "print(\"Embedding model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 documents.\n",
      "Sample Doc: [Project Chimera is a classified top-secret aerospace initiative led by Chief Engineer Dr. Elias Thorne.]\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# --- Step 3: Simulate The PDF Data ---\n",
    "raw_texts = [\n",
    "    # -- The Critical Chain (Split across 3 docs) --\n",
    "    \"Project Chimera is a classified top-secret aerospace initiative led by Chief Engineer Dr. Elias Thorne.\",\n",
    "    \"Dr. Elias Thorne is a renowned physicist who was born and raised in the coastal metropolis of Valoria City.\",\n",
    "    \"Valoria City is an independent economic zone that exclusively trades using the Valorian Credit (V-Cred).\",\n",
    "    \n",
    "    # -- The Noise / Distractors (Designed to trick vector search) --\n",
    "    # 1. Has 'Project', 'Chimera', 'AI' -> High similarity to query part 1\n",
    "    \"Project Chimera-Next is a separate software subsidiary managed by Sarah Connor, focusing on AI.\",\n",
    "    # 2. Has 'Currency', 'Trade' -> High similarity to query part 2\n",
    "    \"The Euro and USD are commonly used in international trade, but not in all independent zones.\",\n",
    "    # 3. Has 'Valoria' but irrelevant context\n",
    "    \"Valoria City is a popular tourist destination known for its beaches, distinct from its economic policies.\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=t) for t in raw_texts]\n",
    "print(f\"Created {len(docs)} documents.\")\n",
    "print(f\"Sample Doc: [{docs[0].page_content}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive_rag_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RUNNING NAIVE RAG ---\n",
      "Query: What is the primary currency used in the city where the lead engineer of Project Chimera was born?\n",
      "\n",
      "Retrieved Context (k=2):\n",
      "1. Project Chimera is a classified top-secret aerospace initiative led by Chief Engineer Dr. Elias Thorne.\n",
      "2. Project Chimera-Next is a separate software subsidiary managed by Sarah Connor, focusing on AI.\n",
      "\n",
      "LLM Response (Naive):\n",
      "Based on the context provided, there is no information about the primary currency used in the city where the lead engineer of Project Chimera was born.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Naive RAG Implementation ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"\\n--- RUNNING NAIVE RAG ---\")\n",
    "query = \"What is the primary currency used in the city where the lead engineer of Project Chimera was born?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 1. Indexing\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 2. Retrieval\n",
    "# We use k=2. The chain is 3 steps long (Project->Person->City->Currency).\n",
    "# Vector search retrieves the 'Project' doc and the 'Distractor Project' doc because they share keywords.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"\\nRetrieved Context (k=2):\")\n",
    "context_str = \"\"\n",
    "for i, d in enumerate(retrieved_docs):\n",
    "    print(f\"{i+1}. {d.page_content}\")\n",
    "    context_str += d.page_content + \"\\n\"\n",
    "\n",
    "# 3. Generation\n",
    "prompt = f\"<|system|>\\nAnswer based ONLY on the context provided. If unsure, say you don't know.\\n<|user|>\\nContext:\\n{context_str}\\nQuestion:\\n{query}\\n<|assistant|>\"\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(\"\\nLLM Response (Naive):\")\n",
    "print(response.split(\"<|assistant|>\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg_extraction_real",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INDUSTRY STANDARD KG EXTRACTION ---\n",
      "Parsing Chunk: Project Chimera is a classified top-secret aerospace initiative led by Chief Engineer Dr. Elias Thorne.\n",
      "   [Raw LLM Output]: Project Chimera | led_by | Dr. Elias Thorne\n",
      "   [Graph Action]: Added Edge (Project Chimera) -> [led_by] -> (Dr. Elias Thorne)\n",
      "\n",
      "Parsing Chunk: Dr. Elias Thorne is a renowned physicist who was born and raised in the coastal metropolis of Valoria City.\n",
      "   [Raw LLM Output]: Dr. Elias Thorne | born_in | Valoria City\n",
      "   [Graph Action]: Added Edge (Dr. Elias Thorne) -> [born_in] -> (Valoria City)\n",
      "\n",
      "Parsing Chunk: Valoria City is an independent economic zone that exclusively trades using the Valorian Credit (V-Cred).\n",
      "   [Raw LLM Output]: Valoria City | uses_currency | Valorian Credit (V-Cred)\n",
      "   [Graph Action]: Added Edge (Valoria City) -> [uses_currency] -> (Valorian Credit (V-Cred))\n",
      "...\n",
      "Graph Statistics: 10 Nodes, 6 Edges\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: ACTUAL Dynamic Knowledge Graph Extraction ---\n",
    "# This is a \"Schema-Based Extraction\" pipeline.\n",
    "# We force the LLM to output a specific separator \"|\" and then parse it.\n",
    "\n",
    "kg = nx.DiGraph()\n",
    "\n",
    "def extract_triplets_with_llm(text):\n",
    "    \"\"\"\n",
    "    Uses the LLM to analyze text and extract structured relations.\n",
    "    We use few-shot prompting to ensure the LLM understands the format.\n",
    "    \"\"\"\n",
    "    extraction_prompt = f\"\"\"<|system|>\n",
    "    You are a Knowledge Graph Engineer. Extract the most important relationship from the sentence.\n",
    "    Format: Subject | Relation | Object\n",
    "    Example: \"Apple Inc. was founded by Steve Jobs.\" -> Apple Inc. | founded_by | Steve Jobs\n",
    "    <|user|>\n",
    "    Sentence: {text}\n",
    "    <|assistant|>\"\"\"\n",
    "    \n",
    "    # 1. Run LLM\n",
    "    raw_out = llm.invoke(extraction_prompt)\n",
    "    # 2. Extract the last part (the assistant's response)\n",
    "    cleaned_out = raw_out.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    # 3. Parse Logic (Robustness check)\n",
    "    parts = []\n",
    "    if \"|\" in cleaned_out:\n",
    "        parts = [p.strip() for p in cleaned_out.split(\"|\")]\n",
    "        \n",
    "    return cleaned_out, parts\n",
    "\n",
    "print(\"\\n--- INDUSTRY STANDARD KG EXTRACTION ---\")\n",
    "\n",
    "# Iterate through our documents and build the graph\n",
    "for doc in docs:\n",
    "    print(f\"\\nParsing Chunk: {doc.page_content[:90]}...\")\n",
    "    \n",
    "    # Call the actual LLM function\n",
    "    raw_response, parsed_parts = extract_triplets_with_llm(doc.page_content)\n",
    "    \n",
    "    if len(parsed_parts) >= 3:\n",
    "        subj, rel, obj = parsed_parts[0], parsed_parts[1], parsed_parts[2]\n",
    "        print(f\"   [Raw LLM Output]: {raw_response}\")\n",
    "        print(f\"   [Graph Action]: Added Edge ({subj}) -> [{rel}] -> ({obj})\")\n",
    "        kg.add_edge(subj, obj, relation=rel)\n",
    "    else:\n",
    "        print(f\"   [Skipped]: LLM output format mismatch. Raw: {raw_response}\")\n",
    "\n",
    "print(f\"\\nGraph Statistics: {kg.number_of_nodes()} Nodes, {kg.number_of_edges()} Edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graph_rag_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RUNNING GRAPH RAG SOLVER ---\n",
      "Query Entity Detected: 'Project Chimera'\n",
      "\n",
      "Traversing Graph Paths...\n",
      "Found Path: Project Chimera is led_by Dr. Elias Thorne. Dr. Elias Thorne is born_in Valoria City. Valoria City is uses_currency Valorian Credit (V-Cred).\n",
      "\n",
      "--- FINAL ANSWER GENERATION ---\n",
      "LLM Graph Response:\n",
      "The primary currency used in the city where the lead engineer of Project Chimera was born is the Valorian Credit (V-Cred).\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Graph RAG Implementation (The Solver) ---\n",
    "\n",
    "print(\"\\n--- RUNNING GRAPH RAG SOLVER ---\")\n",
    "\n",
    "def get_graph_context(graph, query_str, hops=3):\n",
    "    \"\"\"\n",
    "    Instead of searching for text, we search for a connected path.\n",
    "    \"\"\"\n",
    "    # 1. Entity Resolution (Find graph node that matches query)\n",
    "    start_node = None\n",
    "    for node in graph.nodes():\n",
    "        # Simple string matching for demo. In prod, use Vector Similarity or Fuzzy Matching.\n",
    "        if node in query_str and len(node) > 4: \n",
    "            start_node = node\n",
    "            break\n",
    "    \n",
    "    print(f\"Query Entity Detected: '{start_node}'\")\n",
    "    if not start_node: return \"No entity found.\"\n",
    "    \n",
    "    # 2. Traverse (Depth First Search) to gather context\n",
    "    print(\"\\nTraversing Graph Paths...\")\n",
    "    path_sentences = []\n",
    "    \n",
    "    # We use DFS to find connected chains up to depth 3\n",
    "    # This simulates \"Multi-Hop\" reasoning\n",
    "    edges = list(nx.bfs_edges(graph, source=start_node, depth_limit=hops))\n",
    "    \n",
    "    for u, v in edges:\n",
    "        rel = graph[u][v]['relation']\n",
    "        sentence = f\"{u} is {rel} {v}.\"\n",
    "        path_sentences.append(sentence)\n",
    "    \n",
    "    return \" \".join(path_sentences)\n",
    "\n",
    "# 1. Get Context from Graph\n",
    "graph_context = get_graph_context(kg, query, hops=3)\n",
    "print(f\"Found Path: {graph_context}\")\n",
    "\n",
    "# 2. Generate Final Answer\n",
    "print(\"\\n--- FINAL ANSWER GENERATION ---\")\n",
    "prompt = f\"<|system|>\\nAnswer based on the context.\\n<|user|>\\nContext: {graph_context}\\nQuestion: {query}\\n<|assistant|>\"\n",
    "final_response = llm.invoke(prompt)\n",
    "\n",
    "print(\"LLM Graph Response:\")\n",
    "print(final_response.split(\"<|assistant|>\")[-1].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}