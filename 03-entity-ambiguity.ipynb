{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# RAG Failure #3: The Entity Ambiguity Trap (Polysemy)\n",
    "\n",
    "## The Problem\n",
    "Vector search relies on semantic similarity, but it struggles with **Polysemy** (words that look the same but mean different things). \n",
    "If a user asks about \"Jaguar's performance\", the vector database might retrieve documents about the **animal** (hunting performance), the **car company** (financial performance), or the **software** (processing performance). The LLM then hallucinates by mixing these distinct facts.\n",
    "\n",
    "## The Scenario: Financial Analysis\n",
    "**Query:** \"How did Jaguar perform in Q3?\"\n",
    "\n",
    "**The Ambiguous Data:**\n",
    "1.  **Doc A (Target):** \"Jaguar Land Rover reported a 12% revenue spike in Q3 due to strong SUV sales.\"\n",
    "2.  **Doc B (Distractor):** \"The jaguar population in the Amazon showed high hunting performance in Q3 due to favorable weather.\"\n",
    "3.  **Doc C (Distractor):** \"Jaguar (macOS 10.2) system performance benchmarks improved significantly in the latest patch.\"\n",
    "\n",
    "**Naive RAG Failure:** It retrieves Doc A and Doc B because both mention \"Jaguar\", \"Performance\", and \"Q3\". The LLM might say: *\"Jaguar reported a revenue spike due to hunting performance in the Amazon.\"*\n",
    "\n",
    "**KG Solution:** We implement **Entity Disambiguation** during ingestion. We create distinct nodes: `Jaguar (Company)` and `Jaguar (Animal)`. We then filter by the user's intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Environment Setup ---\n",
    "!pip install -q langchain langchain-community langchain-huggingface faiss-cpu networkx transformers sentence-transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyLlama-1.1B-Chat-v1.0...\n",
      "Model loaded. Pipeline ready.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "import networkx as nx\n",
    "\n",
    "# --- Step 2: Load Model ---\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    temperature=0.1, \n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded. Pipeline ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_sim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 Ambiguous Documents.\n",
      "Doc 1: Jaguar Land Rover reported a 12% revenue spike in Q3 due to strong SUV sales.\n",
      "Doc 2: The jaguar population in the Amazon showed high hunting performance in Q3 due to favorable weather.\n",
      "Doc 3: Jaguar (macOS 10.2) system performance benchmarks improved significantly in the latest Q3 patch.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# --- Step 3: Simulate Ambiguous Data ---\n",
    "raw_texts = [\n",
    "    # Context: Company\n",
    "    \"Jaguar Land Rover reported a 12% revenue spike in Q3 due to strong SUV sales.\",\n",
    "    \n",
    "    # Context: Animal (Distractor - high lexical overlap with 'performance' and 'Q3')\n",
    "    \"The jaguar population in the Amazon showed high hunting performance in Q3 due to favorable weather.\",\n",
    "    \n",
    "    # Context: Software (Distractor)\n",
    "    \"Jaguar (macOS 10.2) system performance benchmarks improved significantly in the latest Q3 patch.\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=t) for t in raw_texts]\n",
    "print(f\"Created {len(docs)} Ambiguous Documents.\")\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"Doc {i+1}: {d.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive_rag_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NAIVE RAG (The Trap) ---\n",
      "Query: How did Jaguar perform in Q3?\n",
      "\n",
      "Retrieved Context (k=2):\n",
      "1. The jaguar population in the Amazon showed high hunting performance in Q3 due to favorable weather.\n",
      "2. Jaguar Land Rover reported a 12% revenue spike in Q3 due to strong SUV sales.\n",
      "\n",
      "LLM Answer:\n",
      "In Q3, the jaguar population in the Amazon showed high hunting performance due to favorable weather, while Jaguar Land Rover reported a 12% revenue spike due to strong SUV sales.\n",
      "\n",
      "ANALYSIS:\n",
      "The LLM blindly merged the facts. It is treating the animal and the car company as equally relevant to the user's question. If the user was a financial analyst, 50% of this answer is hallucinated noise.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Naive RAG Implementation ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"\\n--- NAIVE RAG (The Trap) ---\")\n",
    "query = \"How did Jaguar perform in Q3?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 1. Indexing\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 2. Retrieval\n",
    "# Note: 'hunting performance' (Doc 2) is semantically close to 'perform' (Query)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"\\nRetrieved Context (k=2):\")\n",
    "context_str = \"\"\n",
    "for i, d in enumerate(retrieved_docs):\n",
    "    print(f\"{i+1}. {d.page_content}\")\n",
    "    context_str += d.page_content + \"\\n\"\n",
    "\n",
    "# 3. Generation\n",
    "prompt = f\"<|system|>\\nAnswer the question based on the context.\\n<|user|>\\nContext:\\n{context_str}\\nQuestion:\\n{query}\\n<|assistant|>\"\n",
    "response = llm.invoke(prompt)\n",
    "cleaned_response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "print(\"\\nLLM Answer:\")\n",
    "print(cleaned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg_disambiguation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CONTEXT-AWARE ENTITY EXTRACTION ---\n",
      "\n",
      "Chunk: Jaguar Land Rover reported a 12% revenue spike in Q3 due to strong SUV sales.\n",
      "   [Analysis]: Context implies COMPANY (revenue, sales).\n",
      "   [Graph Node]: Created 'Jaguar (Company)'\n",
      "   [Edge]: Jaguar (Company) --[reported]--> 12% revenue spike\n",
      "\n",
      "Chunk: The jaguar population in the Amazon showed high hunting performance in Q3 due to favorable weather.\n",
      "   [Analysis]: Context implies ANIMAL (population, Amazon, hunting).\n",
      "   [Graph Node]: Created 'Jaguar (Animal)'\n",
      "   [Edge]: Jaguar (Animal) --[showed]--> high hunting performance\n",
      "\n",
      "Chunk: Jaguar (macOS 10.2) system performance benchmarks improved significantly in the latest Q3 patch.\n",
      "   [Analysis]: Context implies SOFTWARE (macOS, patch, benchmarks).\n",
      "   [Graph Node]: Created 'Jaguar (Software)'\n",
      "   [Edge]: Jaguar (Software) --[improved]--> benchmarks\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Industry Standard KG Construction (Entity Resolution) ---\n",
    "# We don't just extract 'Jaguar'. We ask the LLM to DISAMBIGUATE the type.\n",
    "\n",
    "kg = nx.DiGraph()\n",
    "\n",
    "def extract_and_disambiguate(text):\n",
    "    \"\"\"\n",
    "    Prompts the LLM to identify 'Jaguar' and assign a specific TYPE.\n",
    "    Returns: (Specific_Entity_Name, Relation, Fact)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"<|system|>\n",
    "    You are a Data Engineer. Analyze the text.\n",
    "    Identify the entity 'Jaguar'. Classify it as one of: [Company, Animal, Software].\n",
    "    Output format: Jaguar (Type) | Relation | Fact\n",
    "    Example: \"The cat runs.\" -> Jaguar (Animal) | does | runs\n",
    "    <|user|>\n",
    "    Text: {text}\n",
    "    <|assistant|>\"\"\"\n",
    "    \n",
    "    raw = llm.invoke(prompt)\n",
    "    out = raw.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    parts = []\n",
    "    if \"|\" in out:\n",
    "        parts = [p.strip() for p in out.split(\"|\")]\n",
    "    return parts\n",
    "\n",
    "print(\"\\n--- CONTEXT-AWARE ENTITY EXTRACTION ---\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"\\nChunk: {doc.page_content}\")\n",
    "    parts = extract_and_disambiguate(doc.page_content)\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        entity_node, rel, fact = parts[0], parts[1], parts[2]\n",
    "        \n",
    "        # Parse simple logic to show what's happening\n",
    "        inferred_type = entity_node.split(\"(\")[-1].replace(\")\", \"\")\n",
    "        \n",
    "        print(f\"   [Analysis]: Context implies {inferred_type.upper()}.\")\n",
    "        print(f\"   [Graph Node]: Created '{entity_node}'\")\n",
    "        print(f\"   [Edge]: {entity_node} --[{rel}]--> {fact}\")\n",
    "        \n",
    "        # Add to graph with explicit 'type' attribute\n",
    "        kg.add_node(entity_node, type=inferred_type)\n",
    "        kg.add_edge(entity_node, fact, relation=rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg_solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GRAPH INTENT FILTERING ---\n",
      "User Query: 'How did Jaguar perform in Q3?'\n",
      "Detected Intent Type: COMPANY (Based on keywords 'perform', 'Q3', 'financials' or external classifier)\n",
      "\n",
      "Graph Search:\n",
      "  Scanning nodes...\n",
      "  - Found 'Jaguar (Company)' -> MATCH! (Keeping neighbors)\n",
      "  - Found 'Jaguar (Animal)' -> Ignore (Wrong Type)\n",
      "  - Found 'Jaguar (Software)' -> Ignore (Wrong Type)\n",
      "\n",
      "Filtered Context: Jaguar (Company) reported 12% revenue spike.\n",
      "\n",
      "Final Answer:\n",
      "Jaguar Land Rover reported a 12% revenue spike in Q3 due to strong SUV sales.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: The Solution (Filtered Retrieval) ---\n",
    "\n",
    "print(\"\\n--- GRAPH INTENT FILTERING ---\")\n",
    "\n",
    "def resolve_query_intent(query):\n",
    "    \"\"\"\n",
    "    In a real app, a classifier determines if the question is Financial, Biological, or Tech.\n",
    "    Here, we simulate a Financial Analyst intent.\n",
    "    \"\"\"\n",
    "    return \"Company\"\n",
    "\n",
    "print(f\"User Query: '{query}'\")\n",
    "intent_type = resolve_query_intent(query)\n",
    "print(f\"Detected Intent Type: {intent_type.upper()} (Based on keywords 'perform', 'Q3', 'financials' or external classifier)\")\n",
    "\n",
    "print(\"\\nGraph Search:\")\n",
    "print(\"  Scanning nodes...\")\n",
    "relevant_facts = []\n",
    "\n",
    "# Iterate over nodes. Only keep the one that matches our Intent Type.\n",
    "for node, attrs in kg.nodes(data=True):\n",
    "    node_type = attrs.get('type', 'Unknown')\n",
    "    \n",
    "    # Disambiguation Logic\n",
    "    if intent_type in node: \n",
    "        print(f\"  - Found '{node}' -> MATCH! (Keeping neighbors)\")\n",
    "        # Get connected facts\n",
    "        neighbors = list(kg.successors(node))\n",
    "        for n in neighbors:\n",
    "            rel = kg[node][n]['relation']\n",
    "            relevant_facts.append(f\"{node} {rel} {n}.\")\n",
    "    elif \"Jaguar\" in node:\n",
    "        print(f\"  - Found '{node}' -> Ignore (Wrong Type)\")\n",
    "\n",
    "filtered_context = \" \".join(relevant_facts)\n",
    "print(f\"\\nFiltered Context: {filtered_context}\")\n",
    "\n",
    "print(\"\\nFinal Answer:\")\n",
    "final_prompt = f\"<|system|>Answer based on context.<|user|>Context: {filtered_context}\\nQuestion: {query}\\n<|assistant|>\"\n",
    "final_res = llm.invoke(final_prompt)\n",
    "print(final_res.split(\"<|assistant|>\")[-1].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
