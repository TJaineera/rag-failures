{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# RAG Failure #4: The Contradictory Information Failure\n",
    "\n",
    "## The Problem\n",
    "Knowledge bases evolve. Old documents (2021 policies) often coexist with new documents (2024 policies) in the vector store. \n",
    "When a user asks **\"What is the policy?\"**, Vector Search retrieves *both* the old and new versions because they are semantically identical. The LLM, unable to distinguish \"truth\" from \"history\", often merges them into a hallucinated mess or hedges its answer.\n",
    "\n",
    "## The Scenario: HR Policy \"Remote Work\" Saga\n",
    "**Query:** \"How many days can I work from home?\"\n",
    "\n",
    "**The Conflicting Data:**\n",
    "1.  **Doc A (2021 Handbook):** \"Under the 'Flex-21' initiative, all employees are entitled to **5 days** of remote work per week.\"\n",
    "2.  **Doc B (2023 Update):** \"Due to RTO mandates, the remote allowance is reduced to **2 days** (hybrid model).\"\n",
    "3.  **Doc C (2024 Memo):** \"Effective immediately, full-time remote work is revoked. Maximum allowance is **0 days** (Strict On-Site).\"\n",
    "\n",
    "**Naive RAG Failure:** It retrieves all three. The LLM says: *\"You can work 5 days, but also 2 days, and currently 0 days.\"*\n",
    "\n",
    "**KG Solution:** We build a **Temporal Graph**. Edges have a `timestamp` property. The query engine sorts facts by date and returns only the latest state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Environment Setup ---\n",
    "!pip install -q langchain langchain-community langchain-huggingface faiss-cpu networkx transformers sentence-transformers accelerate bitsandbytes dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyLlama-1.1B-Chat-v1.0...\n",
      "Model loaded. Pipeline ready.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "import networkx as nx\n",
    "import dateparser # For parsing \"Jan 2021\" into datetime objects\n",
    "\n",
    "# --- Step 2: Load Model ---\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    temperature=0.1, \n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded. Pipeline ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_sim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 Conflicting Documents.\n",
      "Doc 1 (2021): [Dated Jan 1, 2021] Under the 'Flex-21' initiative, all employees are entitled to 5 days of remote work per week.\n",
      "Doc 2 (2023): [Dated June 15, 2023] Update: Due to RTO mandates, the remote allowance is reduced to 2 days (hybrid model).\n",
      "Doc 3 (2024): [Dated Feb 10, 2024] EXECUTIVE MEMO: Effective immediately, full-time remote work is revoked. Maximum allowance is 0 days (Strict On-Site).\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# --- Step 3: Simulate Conflicting Data ---\n",
    "# Note: The dates are embedded in the text, which is common in real PDF parsing.\n",
    "raw_texts = [\n",
    "    \"[Dated Jan 1, 2021] Under the 'Flex-21' initiative, all employees are entitled to 5 days of remote work per week.\",\n",
    "    \"[Dated June 15, 2023] Update: Due to RTO mandates, the remote allowance is reduced to 2 days (hybrid model).\",\n",
    "    \"[Dated Feb 10, 2024] EXECUTIVE MEMO: Effective immediately, full-time remote work is revoked. Maximum allowance is 0 days (Strict On-Site).\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=t) for t in raw_texts]\n",
    "print(f\"Created {len(docs)} Conflicting Documents.\")\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"Doc {i+1} ({['2021', '2023', '2024'][i]}): {d.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive_rag_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NAIVE RAG (The Confusion) ---\n",
      "Query: How many days can I work from home?\n",
      "\n",
      "Retrieved Context (k=3):\n",
      "1. [Dated Feb 10, 2024] EXECUTIVE MEMO: Effective immediately, full-time remote work is revoked. Maximum allowance is 0 days (Strict On-Site).\n",
      "2. [Dated Jan 1, 2021] Under the 'Flex-21' initiative, all employees are entitled to 5 days of remote work per week.\n",
      "3. [Dated June 15, 2023] Update: Due to RTO mandates, the remote allowance is reduced to 2 days (hybrid model).\n",
      "\n",
      "LLM Answer:\n",
      "Based on the provided documents, there are conflicting policies regarding remote work. \n",
      "One document states 5 days, another says 2 days, and a recent memo says 0 days.\n",
      "It is unclear which policy applies to you.\n",
      "\n",
      "ANALYSIS:\n",
      "The LLM acts as a summary engine, not a logic engine. It sees 3 facts and reports 3 facts. \n",
      "It fails to definitively say: \"The answer is 0 days because 2024 > 2023 > 2021.\"\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Naive RAG Implementation ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"\\n--- NAIVE RAG (The Confusion) ---\")\n",
    "query = \"How many days can I work from home?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 1. Indexing\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 2. Retrieval\n",
    "# It retrieves ALL 3 because they are all highly relevant to \"days work from home\".\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"\\nRetrieved Context (k=3):\")\n",
    "context_str = \"\"\n",
    "for i, d in enumerate(retrieved_docs):\n",
    "    print(f\"{i+1}. {d.page_content}\")\n",
    "    context_str += d.page_content + \"\\n\"\n",
    "\n",
    "# 3. Generation\n",
    "prompt = f\"<|system|>\\nAnswer the question based on the context.\\n<|user|>\\nContext:\\n{context_str}\\nQuestion:\\n{query}\\n<|assistant|>\"\n",
    "response = llm.invoke(prompt)\n",
    "cleaned_response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "print(\"\\nLLM Answer:\")\n",
    "print(cleaned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg_temporal_extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEMPORAL KG EXTRACTION ---\n",
      "\n",
      "Parsing Chunk: [Dated Jan 1, 2021] Under the 'Flex-21' initiative, all employees are entitled to 5 days of remote work per week.\n",
      "   [Raw LLM Output]: Remote Work Policy | 5 days | Jan 1, 2021\n",
      "   [Graph Action]: Edge (Remote Work Policy) -> [allowance: 5 days] (valid_from: 2021-01-01)\n",
      "\n",
      "Parsing Chunk: [Dated June 15, 2023] Update: Due to RTO mandates, the remote allowance is reduced to 2 days (hybrid model).\n",
      "   [Raw LLM Output]: Remote Work Policy | 2 days | June 15, 2023\n",
      "   [Graph Action]: Edge (Remote Work Policy) -> [allowance: 2 days] (valid_from: 2023-06-15)\n",
      "\n",
      "Parsing Chunk: [Dated Feb 10, 2024] EXECUTIVE MEMO: Effective immediately, full-time remote work is revoked. Maximum allowance is 0 days (Strict On-Site).\n",
      "   [Raw LLM Output]: Remote Work Policy | 0 days | Feb 10, 2024\n",
      "   [Graph Action]: Edge (Remote Work Policy) -> [allowance: 0 days] (valid_from: 2024-02-10)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Temporal Graph Construction ---\n",
    "# We construct a PROPERTY GRAPH where edges have attributes (timestamps).\n",
    "# We ask the LLM to extract: Topic | Value | Effective_Date\n",
    "\n",
    "kg = nx.DiGraph()\n",
    "\n",
    "def extract_temporal_fact(text):\n",
    "    \"\"\"\n",
    "    Extracts the fact AND the date associated with it.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"<|system|>\n",
    "    You are a Compliance Officer. Extract the Policy Topic, the Allowed Value, and the Effective Date.\n",
    "    Format: Topic | Value | Date\n",
    "    Example: \"[2022] Sick leave is 10 days.\" -> Sick Leave | 10 days | 2022\n",
    "    <|user|>\n",
    "    Text: {text}\n",
    "    <|assistant|>\"\"\"\n",
    "    \n",
    "    raw = llm.invoke(prompt)\n",
    "    out = raw.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    if \"|\" in out:\n",
    "        return [p.strip() for p in out.split(\"|\")]\n",
    "    return []\n",
    "\n",
    "print(\"\\n--- TEMPORAL KG EXTRACTION ---\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"\\nParsing Chunk: {doc.page_content}\")\n",
    "    parts = extract_temporal_fact(doc.page_content)\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        topic, value, date_str = parts[0], parts[1], parts[2]\n",
    "        \n",
    "        # Parse Date String to Python Datetime object for sorting\n",
    "        # In production, use dateparser.parse(date_str)\n",
    "        dt_obj = dateparser.parse(date_str)\n",
    "        \n",
    "        print(f\"   [Raw LLM Output]: {topic} | {value} | {date_str}\")\n",
    "        print(f\"   [Graph Action]: Edge ({topic}) -> [allowance: {value}] (valid_from: {dt_obj.date()})\")\n",
    "        \n",
    "        # Add edge with METADATA\n",
    "        # We use a MultiGraph logic here by appending to a list of facts on the node\n",
    "        # Alternatively, we store edges with keys. Here we simplify: \n",
    "        # Node 'Remote Work Policy' stores a list of historical states.\n",
    "        \n",
    "        if topic not in kg:\n",
    "            kg.add_node(topic, history=[])\n",
    "        \n",
    "        kg.nodes[topic]['history'].append({\n",
    "            \"value\": value,\n",
    "            \"date\": dt_obj,\n",
    "            \"source\": doc.page_content[:20]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg_resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CONFLICT RESOLUTION (Time Travel) ---\n",
      "Query: 'How many days can I work from home?'\n",
      "Identified Topic: 'Remote Work Policy'\n",
      "\n",
      "Found 3 conflicting records. Resolving...\n",
      "  1. Value: 0 days | Date: 2024-02-10 (Latest)\n",
      "  2. Value: 2 days | Date: 2023-06-15\n",
      "  3. Value: 5 days | Date: 2021-01-01\n",
      "\n",
      "Selected Truth: 0 days (Source: [Dated Feb 10, 2024]...)\n",
      "\n",
      "Final Answer:\n",
      "The current remote work allowance is 0 days (Strict On-Site), effective as of Feb 10, 2024.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: The Solution (Deterministic Time Resolution) ---\n",
    "\n",
    "print(\"\\n--- CONFLICT RESOLUTION (Time Travel) ---\")\n",
    "\n",
    "def resolve_latest_truth(topic_query):\n",
    "    \"\"\"\n",
    "    Resolves conflicts by sorting metadata timestamps.\n",
    "    \"\"\"\n",
    "    # 1. Topic Mapping (Simple string match for demo)\n",
    "    target_node = None\n",
    "    for node in kg.nodes():\n",
    "        if \"Remote Work\" in node or \"Work from home\" in topic_query:\n",
    "            target_node = node\n",
    "            break\n",
    "    \n",
    "    print(f\"Query: '{topic_query}'\")\n",
    "    print(f\"Identified Topic: '{target_node}'\")\n",
    "    \n",
    "    if not target_node:\n",
    "        return \"No policy found.\"\n",
    "    \n",
    "    # 2. Retrieve History\n",
    "    history = kg.nodes[target_node]['history']\n",
    "    print(f\"\\nFound {len(history)} conflicting records. Resolving...\")\n",
    "    \n",
    "    # 3. Sort by Date Descending\n",
    "    sorted_history = sorted(history, key=lambda x: x['date'], reverse=True)\n",
    "    \n",
    "    for idx, record in enumerate(sorted_history):\n",
    "        status = \"(Latest)\" if idx == 0 else \"\"\n",
    "        print(f\"  {idx+1}. Value: {record['value']} | Date: {record['date'].date()} {status}\")\n",
    "        \n",
    "    # 4. Return Top Result\n",
    "    latest = sorted_history[0]\n",
    "    print(f\"\\nSelected Truth: {latest['value']} (Source: {latest['source']}...)\")\n",
    "    \n",
    "    return latest\n",
    "\n",
    "latest_fact = resolve_latest_truth(query)\n",
    "\n",
    "print(\"\\nFinal Answer:\")\n",
    "if isinstance(latest_fact, dict):\n",
    "    # We construct the answer programmatically to ensure accuracy\n",
    "    print(f\"The current remote work allowance is {latest_fact['value']}, effective as of {latest_fact['date'].strftime('%b %d, %Y')}.\")\n",
    "else:\n",
    "    print(latest_fact)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
