{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# RAG Failure #12: The Synonymy & Jargon Disconnect\n",
    "\n",
    "## The Problem\n",
    "Organizations often use internal **Code Names**, **Acronyms**, or **Legacy Jargon** that standard Embedding Models (trained on public internet data) do not understand. \n",
    "\n",
    "Even if you retrieve **100% of the documents**, if the user asks about \"The Billing System\" and the logs only mention \"Project Ledger-X\", the LLM will say: *\"I see no mention of a Billing System.\"*\n",
    "\n",
    "## The Scenario: IT Incident Response (CMDB Mapping)\n",
    "**Query:** \"What is the current error rate of the **Checkout Service**?\"\n",
    "\n",
    "**The Jargon-Heavy Logs:**\n",
    "1.  **Log A (The Culprit):** \"**Cart-Flow-V2** is throwing 500 errors. Failure rate is **15%**.\"\n",
    "2.  **Log B (Dependency):** \"**Stripe-Adaptor** latency is normal.\"\n",
    "3.  **Log C (Distractor):** \"The **Checkout** UI team is updating the CSS styles.\"\n",
    "\n",
    "**Naive RAG Failure (Even with Full Context):** \n",
    "The LLM reads all logs. \n",
    "-   It sees \"Cart-Flow-V2\". It doesn't know this IS the Checkout Service.\n",
    "-   It sees \"Checkout UI\". It thinks this is the relevant doc.\n",
    "-   **Result:** *\"The Checkout UI is updating styles. No error rate is mentioned.\"* (Misses the critical incident).\n",
    "\n",
    "**KG Solution:** We load a **Service Registry** into the Graph (`Cart-Flow-V2` --[IMPLEMENTS]--> `Checkout Service`). We expand the user's query to include the technical IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Environment Setup ---\n",
    "!pip install -q langchain langchain-community langchain-huggingface faiss-cpu networkx transformers sentence-transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyLlama-1.1B-Chat-v1.0...\n",
      "Model loaded. Pipeline ready.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "import networkx as nx\n",
    "\n",
    "# --- Step 2: Load Model ---\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    temperature=0.1, \n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded. Pipeline ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_sim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4 Log Documents.\n",
      "Doc 1: [System Log] Service: Cart-Flow-V2 | Status: CRITICAL | Metric: 500 Error Rate is at 15% due to DB timeout.\n",
      "Doc 2: [System Log] Service: Stripe-Adaptor | Status: HEALTHY | Metric: Latency < 20ms.\n",
      "Doc 3: [Dev Team Chat] The Checkout UI team is deploying a new CSS fix for the button color. No functional changes.\n",
      "Doc 4: [Inventory Log] Stock-Keeper-Daemon is processing 50 items per second.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# --- Step 3: Simulate Jargon-Heavy Logs ---\n",
    "# Note the disconnect: User knows \"Checkout Service\". System knows \"Cart-Flow-V2\".\n",
    "raw_texts = [\n",
    "    \"[System Log] Service: Cart-Flow-V2 | Status: CRITICAL | Metric: 500 Error Rate is at 15% due to DB timeout.\",\n",
    "    \"[System Log] Service: Stripe-Adaptor | Status: HEALTHY | Metric: Latency < 20ms.\",\n",
    "    \"[Dev Team Chat] The Checkout UI team is deploying a new CSS fix for the button color. No functional changes.\",\n",
    "    \"[Inventory Log] Stock-Keeper-Daemon is processing 50 items per second.\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=t) for t in raw_texts]\n",
    "print(f\"Created {len(docs)} Log Documents.\")\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"Doc {i+1}: {d.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive_rag_run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NAIVE RAG (Full Retrieval Failure) ---\n",
      "Query: What is the current error rate of the Checkout Service?\n",
      "\n",
      "Retrieved Context (k=4 - ALL DOCS):\n",
      "1. [System Log] Service: Cart-Flow-V2 | Status: CRITICAL | Metric: 500 Error Rate is at 15% due to DB timeout.\n",
      "2. [System Log] Service: Stripe-Adaptor | Status: HEALTHY | Metric: Latency < 20ms.\n",
      "3. [Dev Team Chat] The Checkout UI team is deploying a new CSS fix for the button color. No functional changes.\n",
      "4. [Inventory Log] Stock-Keeper-Daemon is processing 50 items per second.\n",
      "\n",
      "LLM Answer:\n",
      "Based on the context, there is no error rate mentioned for a service explicitly named \"Checkout Service\". The \"Checkout UI team\" is deploying a CSS fix, but no errors are reported for them. There is a 15% error rate for \"Cart-Flow-V2\", but it is not specified if this is the Checkout Service.\n",
      "\n",
      "ANALYSIS:\n",
      "This is a \"Semantic Disconnect\". \n",
      "The LLM has the answer (Doc 1) right in front of it. \n",
      "But because it lacks the domain knowledge that Cart-Flow-V2 == Checkout Service, it refuses to connect the dots.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Naive RAG (Full Context) ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"\\n--- NAIVE RAG (Full Retrieval Failure) ---\")\n",
    "query = \"What is the current error rate of the Checkout Service?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 1. Indexing\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 2. Retrieval\n",
    "# We retrieve ALL documents. The problem isn't missing data, it's missing UNDERSTANDING.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"\\nRetrieved Context (k=4 - ALL DOCS):\")\n",
    "context_str = \"\"\n",
    "for i, d in enumerate(retrieved_docs):\n",
    "    print(f\"{i+1}. {d.page_content}\")\n",
    "    context_str += d.page_content + \"\\n\"\n",
    "\n",
    "# 3. Generation\n",
    "prompt = f\"<|system|>\\nAnswer the question based on the context.\\n<|user|>\\nContext:\\n{context_str}\\nQuestion:\\n{query}\\n<|assistant|>\"\n",
    "response = llm.invoke(prompt)\n",
    "cleaned_response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "print(\"\\nLLM Answer:\")\n",
    "print(cleaned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg_construction_cmdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CMDB INGESTION (Service Registry) ---\n",
      "Simulating loading internal documentation/CMDB...\n",
      "\n",
      "Processing: Cart-Flow-V2 is the backend microservice that handles the Checkout Service logic.\n",
      "   [Mapped]: 'Cart-Flow-V2' is the technical name for 'Checkout Service'\n",
      "   [Graph]: (Cart-Flow-V2) -[IMPLEMENTS]-> (Checkout Service)\n",
      "\n",
      "Processing: Stripe-Adaptor manages the Payment Gateway integration.\n",
      "   [Mapped]: 'Stripe-Adaptor' is the technical name for 'Payment Gateway'\n",
      "   [Graph]: (Stripe-Adaptor) -[IMPLEMENTS]-> (Payment Gateway)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Service Registry (CMDB) Construction ---\n",
    "# We simulate loading a \"Golden Record\" or \"Service Catalog\" that maps jargon to business terms.\n",
    "\n",
    "kg = nx.DiGraph()\n",
    "\n",
    "cmdb_data = [\n",
    "    \"Cart-Flow-V2 is the backend microservice that handles the Checkout Service logic.\",\n",
    "    \"Stripe-Adaptor manages the Payment Gateway integration.\",\n",
    "    \"Stock-Keeper-Daemon is responsible for Inventory Management.\"\n",
    "]\n",
    "\n",
    "def parse_cmdb(text):\n",
    "    \"\"\"\n",
    "    Maps Technical ID -> Business Capability.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"<|system|>\n",
    "    You are an IT Architect. Map the Technical Component to the Business Service.\n",
    "    Format: Tech_ID | IMPLEMENTS | Business_Service\n",
    "    <|user|>\n",
    "    Text: {text}\n",
    "    <|assistant|>\"\"\"\n",
    "    \n",
    "    raw = llm.invoke(prompt)\n",
    "    out = raw.split(\"<|assistant|>\")[-1].strip()\n",
    "    if \"|\" in out:\n",
    "        return [p.strip() for p in out.split(\"|\")]\n",
    "    return []\n",
    "\n",
    "print(\"\\n--- CMDB INGESTION (Service Registry) ---\")\n",
    "print(\"Simulating loading internal documentation/CMDB...\\n\")\n",
    "\n",
    "for entry in cmdb_data:\n",
    "    print(f\"Processing: {entry}\")\n",
    "    parts = parse_cmdb(entry)\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        tech, rel, bus = parts[0], parts[1], parts[2]\n",
    "        print(f\"   [Mapped]: '{tech}' is the technical name for '{bus}'\")\n",
    "        \n",
    "        # Add to Graph\n",
    "        # We treat Business Service as the 'Concept' and Tech ID as the 'Instance'\n",
    "        kg.add_edge(tech, bus, relation=\"IMPLEMENTS\")\n",
    "        print(f\"   [Graph]: ({tech}) -[{rel}]-> ({bus})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg_solution_expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GRAPH-AUGMENTED QUERY EXPANSION ---\n",
      "User Query: \"What is the current error rate of the Checkout Service?\"\n",
      "\n",
      "1. Entity Extraction & Lookup:\n",
      "   Looking for 'Checkout Service' in Graph...\n",
      "   -> Found Node: 'Checkout Service'\n",
      "\n",
      "2. Query Expansion (Reverse Lookup):\n",
      "   Finding technical components that IMPLEMENT 'Checkout Service'...\n",
      "   -> Found: 'Cart-Flow-V2'\n",
      "\n",
      "3. Augmented Search:\n",
      "   New Query Terms: ['Checkout Service', 'Cart-Flow-V2']\n",
      "   Retrieving docs containing ANY of these terms...\n",
      "   - Matched Doc 1: ...Cart-Flow-V2... 15% error rate...\n",
      "\n",
      "4. Final Answer Generation:\n",
      "The Checkout Service (technically 'Cart-Flow-V2') is currently reporting a CRITICAL 15% error rate due to a DB timeout.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: The Solution (Query Expansion) ---\n",
    "\n",
    "print(\"\\n--- GRAPH-AUGMENTED QUERY EXPANSION ---\")\n",
    "print(f\"User Query: \\\"{query}\\\"\")\n",
    "\n",
    "def smart_retrieve(user_query):\n",
    "    # 1. Extract potential entities (Simple keyword match for demo)\n",
    "    print(\"\\n1. Entity Extraction & Lookup:\")\n",
    "    target_concept = \"Checkout Service\" # In prod, use NER\n",
    "    print(f\"   Looking for '{target_concept}' in Graph...\")\n",
    "    \n",
    "    expanded_terms = [target_concept]\n",
    "    \n",
    "    if target_concept in kg:\n",
    "        print(f\"   -> Found Node: '{target_concept}'\")\n",
    "        \n",
    "        # 2. Expand Query\n",
    "        print(\"\\n2. Query Expansion (Reverse Lookup):\")\n",
    "        print(f\"   Finding technical components that IMPLEMENT '{target_concept}'...\")\n",
    "        \n",
    "        # Find predecessors: (Tech) -> (Business)\n",
    "        tech_components = list(kg.predecessors(target_concept))\n",
    "        for t in tech_components:\n",
    "            print(f\"   -> Found: '{t}'\")\n",
    "            expanded_terms.append(t)\n",
    "            \n",
    "    # 3. Filter/Prioritize Documents\n",
    "    print(\"\\n3. Augmented Search:\")\n",
    "    print(f\"   New Query Terms: {expanded_terms}\")\n",
    "    print(\"   Retrieving docs containing ANY of these terms...\")\n",
    "    \n",
    "    relevant_docs = []\n",
    "    for doc in docs:\n",
    "        # Simple string check (simulating vector retrieval of synonyms)\n",
    "        for term in expanded_terms:\n",
    "            if term in doc.page_content:\n",
    "                relevant_docs.append(doc.page_content)\n",
    "                print(f\"   - Matched Doc: {doc.page_content[:45]}...\")\n",
    "                break\n",
    "                \n",
    "    # 4. Generate Answer\n",
    "    print(\"\\n4. Final Answer Generation:\")\n",
    "    final_context = \"\\n\".join(relevant_docs)\n",
    "    final_prompt = f\"<|system|>Answer the query. Note that {expanded_terms[1]} is the {expanded_terms[0]}.<|user|>Context:\\n{final_context}\\nQuestion: {user_query}\\n<|assistant|>\"\n",
    "    \n",
    "    res = llm.invoke(final_prompt)\n",
    "    return res.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "final_answer = smart_retrieve(query)\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
